# Technical Context

**Last Updated**: 2026-02-16

---

## Language: Python

**Rationale**: Every major LLM provider has a first-class Python SDK:
- `anthropic` — Claude (Anthropic)
- `openai` — GPT, o-series (OpenAI)
- `google-genai` — Gemini (Google)
- `mistralai` — Mistral
- `cohere` — Command R (Cohere)
- Local providers (Ollama, LM Studio, vLLM) mostly expose OpenAI-compatible APIs

Python's `asyncio` handles the parallel API call pattern (fan out to multiple models, await all, synthesize). The bottleneck is network latency, not CPU — Python is plenty fast for this.

Rust was considered for single-binary distribution, but the SDK ecosystem is thin. You'd be hand-rolling HTTP clients against every provider's REST API. Maintenance burden not justified.

## Distribution: Docker

**Rationale**: "Install anywhere" without fighting Python packaging. User runs `docker run duh` and they're up. Works on home servers, cloud VMs, Raspberry Pi.

Secondary distribution options (future):
- `pipx` / `uv` for Python-native users
- PyInstaller for single binary (if demand exists)

## Database: SQLAlchemy

**Rationale**: Abstracts the storage backend. User sets a connection string in config:
- `sqlite:///duh.db` — default, zero-config, local
- `postgresql://user:pass@host/db` — scale path, managed hosting
- `mysql://user:pass@host/db` — alternative scale path

Migrations via Alembic. Schema changes don't require changing the database engine.

### Alembic Migrations (v0.2)

- `001_v01_baseline.py` — Thread, Turn, Contribution, TurnSummary, ThreadSummary, Decision tables
- `002_v02_schema.py` — Outcome, Subtask tables; taxonomy fields on Decision (domain, category, tags, complexity)
- `003_v02_votes.py` — Vote table for voting protocol persistence

## CLI UI: Rich / Textual

**Rationale**: Python's `rich` library provides live terminal output — progress bars, parallel task display, streaming. Translates directly to web interface concepts later. `textual` for more complex TUI if needed.

Target CLI experience:
```
Task: Design auth system for mobile app

| PROPOSE ────────────────────────────────
| Proposer: Claude Opus 4.6
| ████████████░░ generating...
└─────────────────────────────────────────

| CHALLENGE (parallel) ───────────────────
| GPT-5.2      ████████████████ done
| Gemini 3 Pro ██████████░░░░░ generating...
| Mistral Large ███████████████ done
└─────────────────────────────────────────

Round 1/3 · 3 models engaged · $0.04 spent
```

## Provider Adapters: Plugin Architecture

**Rationale**: One adapter per provider. Common interface: send prompt, get response, stream tokens. The consensus engine doesn't know or care which provider responded.

Most local providers already speak the OpenAI-compatible API, so the Ollama/LM Studio adapter may just be the OpenAI adapter pointed at a different URL.

## Local Model Protocol: OpenAI-Compatible API

**Rationale**: Ollama, LM Studio, vLLM, text-generation-inference all expose this. One adapter covers most local providers.

## Tool Framework (v0.2)

**Rationale**: Consensus reasoning benefits from grounding in real-world data (web search), code verification (execution), and document access (file read).

- **Tool protocol**: Python `Protocol` class (`src/duh/tools/base.py`) — structural typing, consistent with provider pattern
- **ToolRegistry**: Register tools by name, lookup, list available tools (`src/duh/tools/registry.py`)
- **tool_augmented_send**: Loop that sends to provider, detects tool_calls, executes tools, re-sends with results (`src/duh/tools/augmented_send.py`)
- **Implementations**:
  - `web_search.py` — DuckDuckGo search via `duckduckgo-search>=7.0` package
  - `code_exec.py` — asyncio subprocess execution with timeout and output truncation
  - `file_read.py` — safe file reading with path traversal rejection, binary rejection, 100KB max

## Key Technical Patterns

### Async-First
Everything is `async`/`await`. Model calls, database operations, network requests. The consensus loop fans out to multiple models in parallel and collects responses.

### State Machine for Consensus
The protocol (DECOMPOSE -> PROPOSE -> CHALLENGE -> REVISE -> COMMIT -> NEXT) is a state machine with well-defined transitions, inputs, and outputs per state.

### Voting Protocol (v0.2)
Parallel fan-out to all models, independent responses, majority/weighted aggregation. NOT a state machine — simpler architecture for factual/preference tasks. Auto-classification selects consensus vs voting via `classify_task_type()`.

### Decomposition Protocol (v0.2)
DECOMPOSE state produces `SubtaskSpec` DAG. `schedule_subtasks()` uses `TopologicalSorter` for parallel execution respecting dependencies. `synthesize()` merges results with merge/prioritize strategies.

### Summary Generation
Turn and thread summaries generated by fast/cheap model (Haiku-class or local) after each turn. Thread summary regenerated (not appended) to stay coherent. Raw data always preserved in database.

### Context Window Management
Pass thread summary + recent raw turns + current task context to models each round. Don't pass full history — it exceeds context windows and wastes tokens.

## Decided (v0.3)

- **Project structure**: `src/duh/` with cli/, consensus/, providers/, memory/, config/, core/, tools/, api/, mcp/
- **Testing**: pytest + pytest-asyncio + pytest-cov, 1318 tests, asyncio_mode=auto
- **CI/CD**: GitHub Actions (lint, typecheck, test) + docs deployment to GitHub Pages
- **Provider interface**: `typing.Protocol` (structural typing), stateless adapters
- **5 providers shipping**: Anthropic (3 models), OpenAI (3 models), Google (4 models), Mistral (4 models) — 14 total
- **Memory schema**: SQLAlchemy ORM — Thread, Turn, Contribution, TurnSummary, ThreadSummary, Decision, Outcome, Subtask, Vote, APIKey
- **Configuration**: TOML with Pydantic validation, layered merge (defaults < user < project < env < CLI)
- **Error handling**: DuhError hierarchy with ProviderError, ConsensusError, ConfigError, StorageError
- **Tool framework**: Tool protocol + ToolRegistry + tool_augmented_send, 3 built-in tools
- **Voting**: Parallel fan-out + majority/weighted aggregation, TaskType auto-classification
- **Decomposition**: DECOMPOSE state, TopologicalSorter scheduler, merge/prioritize synthesis
- **Taxonomy**: Classification at COMMIT time via structured output, domain/category/tags/complexity
- **Outcome tracking**: Feedback CLI, outcome context injection in future rounds
- **REST API**: FastAPI app factory, API key auth + rate limiting, 8 endpoints + WebSocket
- **MCP server**: Direct Python calls (no REST dependency), 3 tools (duh_ask, duh_recall, duh_threads)
- **Python client**: `duh-client` package in `client/`, async httpx + sync wrappers
- **Documentation**: MkDocs Material, deployed to GitHub Pages
- **50 source files**, mypy strict clean, ruff clean

## Dependencies (v0.2+v0.3)

- `duckduckgo-search>=7.0` — web search tool implementation
- `fastapi>=0.115` — REST API framework, OpenAPI spec auto-generation
- `uvicorn[standard]>=0.30` — ASGI server for `duh serve`
- `httpx>=0.27` — async HTTP client for Python client library
- `mistralai>=1.0` — Mistral provider SDK (5th cloud provider)
- `mcp>=1.0` — MCP server SDK for AI agent integration

## Not Yet Decided

- Network protocol for federated sharing (post-1.0)
- Vector search solution for SQLite (v1.0)
- Client library packaging: monorepo `client/` dir vs separate repo
- MCP server transport: stdio vs SSE vs streamable HTTP
