# Technical Context

**Last Updated**: 2026-02-15

---

## Language: Python

**Rationale**: Every major LLM provider has a first-class Python SDK:
- `anthropic` — Claude (Anthropic)
- `openai` — GPT, o-series (OpenAI)
- `google-genai` — Gemini (Google)
- `mistralai` — Mistral
- `cohere` — Command R (Cohere)
- Local providers (Ollama, LM Studio, vLLM) mostly expose OpenAI-compatible APIs

Python's `asyncio` handles the parallel API call pattern (fan out to multiple models, await all, synthesize). The bottleneck is network latency, not CPU — Python is plenty fast for this.

Rust was considered for single-binary distribution, but the SDK ecosystem is thin. You'd be hand-rolling HTTP clients against every provider's REST API. Maintenance burden not justified.

## Distribution: Docker

**Rationale**: "Install anywhere" without fighting Python packaging. User runs `docker run duh` and they're up. Works on home servers, cloud VMs, Raspberry Pi.

Secondary distribution options (future):
- `pipx` / `uv` for Python-native users
- PyInstaller for single binary (if demand exists)

## Database: SQLAlchemy

**Rationale**: Abstracts the storage backend. User sets a connection string in config:
- `sqlite:///duh.db` — default, zero-config, local
- `postgresql://user:pass@host/db` — scale path, managed hosting
- `mysql://user:pass@host/db` — alternative scale path

Migrations via Alembic. Schema changes don't require changing the database engine.

## CLI UI: Rich / Textual

**Rationale**: Python's `rich` library provides live terminal output — progress bars, parallel task display, streaming. Translates directly to web interface concepts later. `textual` for more complex TUI if needed.

Target CLI experience:
```
Task: Design auth system for mobile app

| PROPOSE ────────────────────────────────
| Proposer: Claude Opus 4.6
| ████████████░░ generating...
└─────────────────────────────────────────

| CHALLENGE (parallel) ───────────────────
| GPT-5.2      ████████████████ done
| Gemini 3 Pro ██████████░░░░░ generating...
| Mistral Large ███████████████ done
└─────────────────────────────────────────

Round 1/3 · 3 models engaged · $0.04 spent
```

## Provider Adapters: Plugin Architecture

**Rationale**: One adapter per provider. Common interface: send prompt, get response, stream tokens. The consensus engine doesn't know or care which provider responded.

Most local providers already speak the OpenAI-compatible API, so the Ollama/LM Studio adapter may just be the OpenAI adapter pointed at a different URL.

## Local Model Protocol: OpenAI-Compatible API

**Rationale**: Ollama, LM Studio, vLLM, text-generation-inference all expose this. One adapter covers most local providers.

## Key Technical Patterns

### Async-First
Everything is `async`/`await`. Model calls, database operations, network requests. The consensus loop fans out to multiple models in parallel and collects responses.

### State Machine for Consensus
The protocol (DECOMPOSE → PROPOSE → CHALLENGE → REVISE → COMMIT → NEXT) is a state machine with well-defined transitions, inputs, and outputs per state.

### Summary Generation
Turn and thread summaries generated by fast/cheap model (Haiku-class or local) after each turn. Thread summary regenerated (not appended) to stay coherent. Raw data always preserved in database.

### Context Window Management
Pass thread summary + recent raw turns + current task context to models each round. Don't pass full history — it exceeds context windows and wastes tokens.

## Not Yet Decided

- Exact project structure (monorepo? packages?)
- Testing framework (pytest assumed)
- CI/CD approach
- Provider adapter interface definition (abstract base class? protocol?)
- Memory schema details (SQLAlchemy model definitions)
- Configuration format (TOML? YAML? JSON?)
- Logging framework
- Error handling patterns
- API design for web/MCP/A2A interfaces (Phase 3)
- Network protocol for federated sharing (Phase 4)
