# Technical Context

**Last Updated**: 2026-02-15

---

## Language: Python

**Rationale**: Every major LLM provider has a first-class Python SDK:
- `anthropic` — Claude (Anthropic)
- `openai` — GPT, o-series (OpenAI)
- `google-genai` — Gemini (Google)
- `mistralai` — Mistral
- `cohere` — Command R (Cohere)
- Local providers (Ollama, LM Studio, vLLM) mostly expose OpenAI-compatible APIs

Python's `asyncio` handles the parallel API call pattern (fan out to multiple models, await all, synthesize). The bottleneck is network latency, not CPU — Python is plenty fast for this.

Rust was considered for single-binary distribution, but the SDK ecosystem is thin. You'd be hand-rolling HTTP clients against every provider's REST API. Maintenance burden not justified.

## Distribution: Docker

**Rationale**: "Install anywhere" without fighting Python packaging. User runs `docker run duh` and they're up. Works on home servers, cloud VMs, Raspberry Pi.

Secondary distribution options (future):
- `pipx` / `uv` for Python-native users
- PyInstaller for single binary (if demand exists)

## Database: SQLAlchemy

**Rationale**: Abstracts the storage backend. User sets a connection string in config:
- `sqlite:///duh.db` — default, zero-config, local
- `postgresql://user:pass@host/db` — scale path, managed hosting
- `mysql://user:pass@host/db` — alternative scale path

Migrations via Alembic. Schema changes don't require changing the database engine.

## CLI UI: Rich / Textual

**Rationale**: Python's `rich` library provides live terminal output — progress bars, parallel task display, streaming. Translates directly to web interface concepts later. `textual` for more complex TUI if needed.

Target CLI experience:
```
Task: Design auth system for mobile app

| PROPOSE ────────────────────────────────
| Proposer: Claude Opus 4.6
| ████████████░░ generating...
└─────────────────────────────────────────

| CHALLENGE (parallel) ───────────────────
| GPT-5.2      ████████████████ done
| Gemini 3 Pro ██████████░░░░░ generating...
| Mistral Large ███████████████ done
└─────────────────────────────────────────

Round 1/3 · 3 models engaged · $0.04 spent
```

## Provider Adapters: Plugin Architecture

**Rationale**: One adapter per provider. Common interface: send prompt, get response, stream tokens. The consensus engine doesn't know or care which provider responded.

Most local providers already speak the OpenAI-compatible API, so the Ollama/LM Studio adapter may just be the OpenAI adapter pointed at a different URL.

## Local Model Protocol: OpenAI-Compatible API

**Rationale**: Ollama, LM Studio, vLLM, text-generation-inference all expose this. One adapter covers most local providers.

## Key Technical Patterns

### Async-First
Everything is `async`/`await`. Model calls, database operations, network requests. The consensus loop fans out to multiple models in parallel and collects responses.

### State Machine for Consensus
The protocol (DECOMPOSE → PROPOSE → CHALLENGE → REVISE → COMMIT → NEXT) is a state machine with well-defined transitions, inputs, and outputs per state.

### Summary Generation
Turn and thread summaries generated by fast/cheap model (Haiku-class or local) after each turn. Thread summary regenerated (not appended) to stay coherent. Raw data always preserved in database.

### Context Window Management
Pass thread summary + recent raw turns + current task context to models each round. Don't pass full history — it exceeds context windows and wastes tokens.

## Decided (v0.1)

- **Project structure**: `src/duh/` with cli/, consensus/, providers/, memory/, config/, core/
- **Testing**: pytest + pytest-asyncio + pytest-cov, 703 tests, asyncio_mode=auto
- **CI/CD**: GitHub Actions (lint, typecheck, test) + docs deployment to GitHub Pages
- **Provider interface**: `typing.Protocol` (structural typing), stateless adapters
- **Memory schema**: SQLAlchemy ORM — Thread, Turn, Contribution, TurnSummary, ThreadSummary, Decision
- **Configuration**: TOML with Pydantic validation, layered merge (defaults < user < project < env < CLI)
- **Error handling**: DuhError hierarchy with ProviderError, ConsensusError, ConfigError, StorageError
- **3 providers shipping**: Anthropic (3 models), OpenAI (3 models), Google (4 models) — 10 total
- **Documentation**: MkDocs Material, 19 pages, deployed to GitHub Pages

## Not Yet Decided

- API design for web/MCP/A2A interfaces (v0.3)
- Network protocol for federated sharing (post-1.0)
- Vector search solution for SQLite (v1.0)
