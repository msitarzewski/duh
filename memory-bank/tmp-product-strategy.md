# duh — Product Strategy & Versioned Release Plan

**Date**: 2026-02-15
**Author**: Product Strategist (Agent Team)
**Status**: Draft for team review

---

## Executive Summary

duh is a multi-model consensus and knowledge infrastructure. The path from concept to product requires shipping usable value early, validating the core thesis (multi-model consensus produces better outputs than single models), and building toward the network effects that create a defensible moat.

The release plan follows a principle: **each version must be independently useful**. No version should feel like an incomplete stepping stone. A user at 0.1.0 gets value. A user at 0.5.0 gets significantly more. By 1.0.0, the full vision is operational.

---

## 1. MVP Definition: What Proves the Thesis?

The minimum viable product must answer one question: **Does structured multi-model consensus produce noticeably better results than asking a single model?**

If the answer is "not really" or "only marginally," the entire project thesis is invalidated. So the MVP must:

1. Run a real consensus loop (PROPOSE, CHALLENGE, REVISE, COMMIT) across 2+ models from different providers
2. Show the user the debate — what each model proposed, how others challenged it, what changed
3. Persist the result so the user can recall it later
4. Feel faster/better/more insightful than just asking Claude or GPT directly

What the MVP does NOT need:
- Task decomposition (use single-task consensus first)
- Network/federation
- Web interface
- More than 2-3 providers
- Outcome tracking
- Cost optimization

**The MVP is v0.1.0.**

---

## 2. Versioned Release Plan

### v0.1.0 — "It Works" (MVP)
**Theme**: Prove the consensus thesis with a working CLI tool.

**What ships**:
- Provider adapters: **Anthropic (Claude)** and **OpenAI (GPT)** — the two most widely held API keys
- OpenAI-compatible adapter for **Ollama** (local models) — because local-first is a core tenet, not a Phase 2 feature
- Consensus state machine: PROPOSE → CHALLENGE → REVISE → COMMIT (single task, no decomposition)
- SQLite memory: threads, turns, contributions stored persistently
- Turn summaries generated by cheapest available model
- Rich CLI showing parallel execution, streaming, model responses
- Configuration via TOML file (provider API keys, model selection, default models per role)
- Basic cost tracking: tokens used per model per turn, total cost per thread

**What a user can do**:
```
$ duh "Should I use a microservices or monolith architecture for a new SaaS product?"
```
The system fans out to Claude + GPT (+ local model if configured), runs a structured debate, and returns a synthesized answer with preserved dissent. The user sees the debate happen in real time. The result is saved and recallable.

**Provider order rationale**:
- Anthropic first: Best reasoning model (Claude Opus), project creator likely has key
- OpenAI second: Largest install base, most users have keys, GPT-4o/o3 provide genuine diversity of perspective
- Ollama third: Proves local-first commitment from day one, Llama 3.3 70B is good enough to meaningfully challenge cloud models on many topics

**Acceptance criteria**:
- [ ] `duh "question"` runs full consensus loop and returns result
- [ ] User sees real-time streaming of each model's contribution
- [ ] CHALLENGE phase uses forced disagreement prompts (not "do you agree?")
- [ ] Result stored in SQLite, recallable via `duh history`
- [ ] Works with any 2 of the 3 providers (graceful degradation if one isn't configured)
- [ ] Cost displayed after each thread
- [ ] `duh config` for setup

**User value**: "I get better answers by hearing multiple AI models debate than by asking one."

---

### v0.2.0 — "It Remembers"
**Theme**: Knowledge accumulation begins. The system gets smarter over time.

**What ships**:
- Full memory schema: decisions, patterns (Layer 2 institutional memory)
- Thread summaries: rolling regeneration after each turn (not just append)
- `duh recall "topic"` — search past decisions by keyword/topic
- Decision extraction: after COMMIT, structured decision records are created automatically
- Context injection: when starting a new thread, relevant past decisions are surfaced to models
- Thread continuation: `duh continue <thread-id>` to revisit and extend past reasoning
- Google Gemini adapter (third cloud provider — adds genuine model diversity)
- Configurable consensus rounds (default 2, user can set 1-5)

**What a user can do**:
```
$ duh "What database should I use for this IoT project?"
# ... consensus runs, decision stored ...

# Two weeks later:
$ duh "I need to add time-series data to the IoT project"
# System automatically surfaces the prior database decision as context
```

**Provider order rationale**:
- Gemini added here: Google's models offer genuinely different training data and reasoning style. Three cloud providers + local gives real diversity.

**Acceptance criteria**:
- [ ] Decisions automatically extracted and stored after each thread
- [ ] `duh recall` returns relevant past decisions with summaries
- [ ] New threads receive relevant past context automatically (semantic match)
- [ ] Thread summaries regenerate (not just append) for coherence
- [ ] Gemini participates in consensus alongside Claude, GPT, and local models

**User value**: "duh remembers what we've discussed before. It builds on past reasoning instead of starting from scratch every time."

---

### v0.3.0 — "It Thinks Deeper"
**Theme**: Task decomposition and multi-step reasoning for complex topics.

**What ships**:
- DECOMPOSE phase: strong reasoner model breaks complex concepts into subtasks
- Sequential consensus: each subtask runs its own PROPOSE → CHALLENGE → REVISE → COMMIT
- Subtask dependency awareness: later subtasks receive context from earlier ones
- Synthesis phase: final rollup of all subtask decisions into coherent output
- Outcome tracking: `duh outcome <thread-id> --result "it worked"` / `--result "failed because X"`
- Outcome feedback displayed in future relevant threads
- Mistral adapter (fourth cloud provider)
- `duh models` — list configured models with capabilities, context windows, costs
- Parallel subtask execution where dependencies allow

**What a user can do**:
```
$ duh "Design a go-to-market strategy for a B2B fintech product targeting mid-market CFOs"
# System decomposes into: market analysis, positioning, channel strategy,
# pricing, launch timeline — each gets its own consensus round
# Final synthesis brings it all together
```

**Provider order rationale**:
- Mistral added here: Strong European model, good at structured reasoning, adds genuine perspective diversity. By v0.3.0 users have 4 cloud + local options.

**Acceptance criteria**:
- [ ] Complex queries automatically decomposed into subtasks
- [ ] Each subtask runs full consensus independently
- [ ] Subtask results flow as context to dependent subtasks
- [ ] Final synthesis coherently combines all subtask outputs
- [ ] `duh outcome` records and surfaces outcome data
- [ ] Users can see the decomposition tree and drill into any subtask

**User value**: "duh handles complex, multi-faceted questions I wouldn't trust to a single model. It breaks problems down and thinks through each piece."

---

### v0.4.0 — "It's Accessible"
**Theme**: API layer opens duh to integrations and programmatic use.

**What ships**:
- REST API: all CLI functionality exposed as HTTP endpoints
- WebSocket support for streaming consensus in real time
- API key authentication (local, self-managed)
- Python client library (`pip install duh-client`)
- MCP server implementation (duh as a tool for AI agents)
- Cohere adapter (fifth cloud provider)
- `duh export <thread-id> --format json|markdown` for sharing results
- Batch mode: `duh batch questions.txt` for running multiple queries

**What a user can do**:
```python
from duh import DuhClient

client = DuhClient("http://localhost:8080")
result = await client.ask("Should we migrate to Kubernetes?",
                          models=["claude-opus", "gpt-4o", "local/llama3.3"])
print(result.decision)
print(result.dissent)
```

**Acceptance criteria**:
- [ ] Full REST API with OpenAPI spec
- [ ] WebSocket streaming works for real-time consensus display
- [ ] MCP server exposes duh as tools for other AI agents
- [ ] Python client library published to PyPI
- [ ] API authentication and rate limiting functional

**User value**: "I can integrate duh into my workflows, scripts, and other tools. Other AI agents can use duh as a reasoning tool."

---

### v0.5.0 — "It Has a Face"
**Theme**: Web interface makes consensus visible and shareable.

**What ships**:
- Web UI: real-time consensus visualization (models debating, challenging, revising)
- Thread browser: search, filter, browse past threads and decisions
- Decision explorer: drill into any decision's reasoning, dissent, outcome history
- Share links: generate read-only links to specific threads/decisions
- User preferences: default models, cost thresholds, consensus depth
- Mobile-responsive design
- Docker Compose with web UI + API as default deployment

**What a user can do**:
Open a browser, type a question, watch AI models debate it in real time with a visual display showing proposals flowing into challenges flowing into revisions. Browse past decisions. Share a link to a particularly insightful debate with a colleague.

**Acceptance criteria**:
- [ ] Web UI shows real-time consensus with streaming from each model
- [ ] Thread history is browsable and searchable
- [ ] Decisions show full provenance (which model proposed what, who challenged, how it was resolved)
- [ ] Share links work without authentication (read-only)
- [ ] Docker Compose up-and-running experience works in under 2 minutes

**User value**: "I can see and share the thinking process. The web interface makes consensus tangible and collaborative."

---

### v0.6.0 — "It Connects"
**Theme**: Federated knowledge sharing begins. Instances start talking to each other.

**What ships**:
- Knowledge navigator protocol v1 (gRPC or HTTP-based)
- Navigator node: lightweight service that indexes published decisions from connected instances
- `duh publish <thread-id>` — opt-in publishing of a decision to configured navigators
- `duh search --network "topic"` — query navigators for relevant decisions from other instances
- Network-informed consensus: published decisions surface as context during new threads
- Trust model v1: outcome-based quality signals (decisions that led to good outcomes rank higher)
- Privacy controls: per-thread, per-navigator publish settings
- Navigator discovery: manual configuration (no auto-discovery yet)

**What a user can do**:
```
$ duh publish 42 --navigator https://nav.example.com
# Decision #42 is now discoverable by others on that navigator

$ duh "How should I structure database migrations for a Django project?"
# System queries configured navigators, finds 3 relevant prior decisions from other instances
# Models receive this network knowledge as additional context
```

**Acceptance criteria**:
- [ ] Navigator node can be run independently (`docker run duh-navigator`)
- [ ] Instances publish decisions to navigators with metadata (topic, confidence, outcome)
- [ ] Network search returns relevant decisions with provenance
- [ ] Models receive network knowledge during consensus
- [ ] Privacy controls work: nothing shared without explicit opt-in
- [ ] Quality signals: decisions with positive outcomes surface higher

**User value**: "My instance benefits from what other instances have figured out. I'm not starting from scratch — the network has already thought about this."

---

### v0.7.0 — "It Scales"
**Theme**: Production hardening, multi-user, performance.

**What ships**:
- Multi-user support: user accounts, per-user threads and decisions
- Role-based access: admin, contributor, viewer
- PostgreSQL as recommended production database (SQLite still supported)
- Connection pooling, query optimization, caching layer
- Rate limiting per user and per provider
- Horizontal scaling: multiple API workers behind load balancer
- Health checks, metrics endpoint (Prometheus-compatible)
- Backup/restore utilities
- A2A protocol support (agent-to-agent, for tool ecosystems)

**Acceptance criteria**:
- [ ] Multi-user with authentication works
- [ ] PostgreSQL deployment documented and tested
- [ ] Performance: consensus round completes within model latency + 500ms overhead
- [ ] Metrics and monitoring operational
- [ ] Backup/restore tested

**User value**: "I can run duh for my team/organization, not just myself."

---

### v0.8.0 — "It Knows"
**Theme**: The knowledge base emerges as a browsable, searchable resource.

**What ships**:
- Knowledge base UI: browsable collection of consensus-validated decisions
- Every entry shows: resolved position, debate history, dissent, outcome track record
- Category/tag system for organizing knowledge entries
- Search across local + network knowledge
- Knowledge quality indicators (how many models agreed, outcome success rate, how many instances corroborated)
- Fact-checking mode: `duh verify "claim"` — structured claim decomposition and multi-model verification
- Citation and source tracking

**Acceptance criteria**:
- [ ] Knowledge base is browsable by topic, date, quality score
- [ ] Each entry shows full debate provenance
- [ ] Fact-checking mode decomposes claims and verifies sub-claims
- [ ] Network knowledge integrates into the browsable interface

**User value**: "I have a growing encyclopedia of AI-debated, evidence-backed knowledge that I can browse, search, and trust more than any single model's output."

---

### v0.9.0 — "It Researches"
**Theme**: Research mode for deep, multi-model investigation.

**What ships**:
- Research mode: `duh research "topic"` — extended multi-model investigation
- Research produces a structured report, not just a decision
- Models approach from different angles (assigned perspectives: technical, economic, ethical, historical)
- Iterative deepening: research can spawn sub-investigations
- Export research as formatted reports (Markdown, PDF)
- Research threads can be collaborative (multiple users contributing direction)
- Living documents: research can be reopened and extended with new models/data

**Acceptance criteria**:
- [ ] Research mode produces structured, multi-perspective reports
- [ ] Reports are exportable in Markdown and PDF
- [ ] Research threads support iterative deepening
- [ ] Collaborative direction-setting works for multi-user instances

**User value**: "duh is my research partner. It investigates topics from multiple angles and produces reports that no single model could."

---

### v1.0.0 — "duh."
**Theme**: The full vision. Stable, documented, production-ready.

**What ships**:
- Stable APIs (REST, WebSocket, MCP, A2A)
- Comprehensive documentation: user guide, API reference, self-hosting guide, navigator operator guide
- Plugin system for custom provider adapters
- Navigator auto-discovery protocol
- Multiple navigator support (general-purpose and specialized)
- Knowledge base federation: navigators can index and cross-reference each other
- Semantic search via vector embeddings (Layer 3 memory)
- Long-term stability guarantees: migration paths, backward compatibility
- Performance benchmarks published
- Security audit completed

**Acceptance criteria**:
- [ ] All APIs stable with semantic versioning commitment
- [ ] Documentation covers all user paths
- [ ] Federation works across multiple navigators
- [ ] Vector search operational for semantic recall
- [ ] Security audit passed
- [ ] Performance benchmarks published

**User value**: "A complete thinking infrastructure — multi-model consensus, persistent knowledge, federated network, browsable knowledge base. The answer to 'why wouldn't you use all of them?' is... duh."

---

## 3. Milestones Summary

| Version | Theme | Key Deliverable | Build Phase |
|---------|-------|-----------------|-------------|
| 0.1.0 | It Works | Working consensus CLI | Phase 1 |
| 0.2.0 | It Remembers | Knowledge accumulation | Phase 1-2 |
| 0.3.0 | It Thinks Deeper | Task decomposition + outcomes | Phase 2 |
| 0.4.0 | It's Accessible | API + MCP | Phase 3 |
| 0.5.0 | It Has a Face | Web UI | Phase 3 |
| 0.6.0 | It Connects | Federated knowledge | Phase 4 |
| 0.7.0 | It Scales | Multi-user, production | Phase 3-4 |
| 0.8.0 | It Knows | Browsable knowledge base | Phase 5 |
| 0.9.0 | It Researches | Research mode | Phase 5 |
| 1.0.0 | duh. | Full vision, stable | Phase 5 |

---

## 4. Early Adopter Profiles

### Wedge User: The "AI Power User"
**Who**: People who already have API keys for 2+ providers. They've tried Claude, GPT, Gemini and noticed they give different answers. They manually copy-paste between models to compare. They're frustrated that no tool lets them do this systematically.

**Where to find them**: Hacker News, r/LocalLLaMA, AI Twitter/X, developer communities (even though duh isn't dev-only, devs adopt CLI tools first)

**Why they adopt**: They're already doing manual consensus. duh automates what they do by hand.

**v0.1.0 is enough for them.** They just need the consensus loop to work.

### Second Wave: The "Local Model Enthusiast"
**Who**: r/LocalLLaMA community. People running Ollama on beefy hardware. They want their local models to be more useful but know they can't match cloud model quality alone.

**Why they adopt**: Local models as first-class citizens. Their Llama 3.3 70B participates equally alongside Claude Opus. And with network knowledge (v0.6.0+), their zero-cost local instance benefits from cloud-generated knowledge.

**v0.1.0 gets them in the door. v0.6.0 is the killer feature.**

### Third Wave: The "Knowledge Worker"
**Who**: Researchers, analysts, consultants, strategists. People whose job is synthesizing information and making decisions. Not necessarily technical.

**Why they adopt**: The web interface (v0.5.0) makes consensus accessible without CLI skills. The knowledge base (v0.8.0) gives them a browsable resource of AI-debated conclusions.

**v0.5.0 is the entry point. v0.8.0 is the retention hook.**

### Fourth Wave: Teams and Organizations
**Who**: Teams that need shared decision-making infrastructure. Could be engineering teams, research groups, consulting firms, or policy organizations.

**Why they adopt**: Multi-user support (v0.7.0), shared knowledge base, institutional memory that persists beyond individual contributors.

**v0.7.0 is the entry point.**

### Long-term: The "Curious Everyone"
**Who**: Domain-agnostic. Farmers, small business owners, students, hobbyists. People with questions who want better-than-single-model answers.

**Why they adopt**: The knowledge base (v0.8.0+) and research mode (v0.9.0) become the interface — they may never run a consensus themselves but browse and benefit from the accumulated knowledge.

**v0.8.0+ is the entry point.** Network effects make this viable.

---

## 5. Go-to-Market Strategy

### Phase 1: Open Source Launch (v0.1.0)

**Positioning**: "Why wouldn't you use all of them? ...duh."

The name is the marketing. It's memorable, slightly provocative, and self-explanatory once you understand what it does.

**Launch channels**:
1. **Hacker News** — "Show HN: duh — multi-model consensus engine for AI" — target the AI power user community
2. **r/LocalLLaMA** — "Your local models can now participate in structured debates with cloud models" — target local model enthusiasts
3. **GitHub README** — The README IS the pitch. Show the CLI output. Show the debate. Show the dissent. Make it visceral.
4. **Twitter/X thread** — Live demo: "I asked the same question to Claude, GPT, and Llama. Then I asked duh. Here's what happened." Side-by-side comparison.
5. **Blog post** — "The case for multi-model consensus: why asking one AI isn't enough"

**What NOT to do at launch**:
- Don't position as a developer tool (limits audience)
- Don't lead with "agent framework" (crowded, commoditized term)
- Don't emphasize federation/network (too early, nothing to federate)
- Don't compare to LangChain/CrewAI (different category — those are workflow tools, duh is a thinking tool)

**Lead with**:
- The debate visualization (the CLI showing models arguing is inherently compelling)
- Concrete examples where consensus produces a better answer than any single model
- The "show the work" ethos — transparency of reasoning, preserved dissent

### Phase 2: Community Building (v0.1.0 - v0.3.0)

**Discord/community server** from day one. Key channels:
- `#show-your-debates` — users share interesting consensus outputs
- `#provider-adapters` — community contributions for new providers
- `#feature-requests` — direct user input into roadmap
- `#local-models` — dedicated space for the local model community

**Content strategy**:
- Weekly "Debate of the Week" — run an interesting question through duh, publish the full debate with commentary
- Monthly "Consensus Report" — pick a trending topic, run it through multiple model combinations, publish findings
- Provider comparison content — "How Claude, GPT, and Gemini disagree about X" (drives organic search)

**Contributor funnel**:
- Provider adapters are the easiest contribution (well-defined interface, self-contained)
- Challenge prompt engineering (improving the CHALLENGE phase prompts)
- Documentation and examples

### Phase 3: Web Interface Launch (v0.5.0)

**Major launch moment**. This is when non-technical users can engage.

**Launch strategy**:
- Hosted demo instance: `try.duh.dev` — free, rate-limited, pre-configured with multiple providers
- Video demo: screen recording of a consensus debate happening in real time in the web UI
- "Share your debate" feature: every consensus thread gets a shareable link, enabling viral distribution
- Blog post: "Watch AI models argue about [timely topic]"

### Phase 4: Network Launch (v0.6.0)

**The "network moment."** This is the most ambitious marketing beat — it's when duh becomes more than a tool and starts becoming infrastructure.

**Launch strategy**:
- Run the first public navigator node: `nav.duh.dev`
- Seed it with 100+ high-quality consensus debates on diverse topics
- Blog post: "What happens when AI instances share what they've learned"
- Reach out to universities, research groups, think tanks who might run their own navigators

### Phase 5: Knowledge Base (v0.8.0+)

**The "Wikipedia moment."** This is the long-term defensible asset.

**Strategy**:
- Launch `kb.duh.dev` — browsable knowledge base, SEO-optimized
- Every knowledge entry is a landing page (organic search traffic)
- "Unlike Wikipedia: see the debate, not just the conclusion"
- Fact-checking mode gets its own marketing push: "AI fact-checking where you can see the reasoning"

---

## 6. Release Cadence

### Recommended Cadence

| Version Range | Cadence | Rationale |
|---------------|---------|-----------|
| 0.1.0 - 0.3.0 | 3-4 weeks per release | Core functionality. Ship fast, get feedback. These are the "does the thesis hold?" releases. |
| 0.4.0 - 0.5.0 | 4-6 weeks per release | API and web UI are larger surface areas. Need more testing. |
| 0.6.0 - 0.7.0 | 6-8 weeks per release | Federation and scaling are genuinely hard problems. Don't rush. |
| 0.8.0 - 1.0.0 | 6-8 weeks per release | Knowledge base and research mode need polish. 1.0 needs stability bake time. |

**Total estimated timeline**: 10-14 months from start to 1.0.0.

**Patch releases** (0.x.1, 0.x.2): As needed for bug fixes, shipped within days.

**Critical rule**: Never delay a release to cram in one more feature. Ship what's ready. The version after this one is only 3-4 weeks away.

### Pre-release Milestones Within Each Version

Each version should hit these internal milestones:
1. **Alpha** (week 1-2): Core feature implemented, rough edges, internal testing only
2. **Beta** (week 2-3): Feature complete, community testing, bug fixes
3. **RC** (week 3-4): Release candidate, documentation complete, final testing
4. **Release**: Tagged, Docker image published, changelog published

---

## 7. Pricing & Monetization (Future Considerations)

### Phase 1 Strategy: Pure Open Source (v0.1.0 - v0.7.0)

No monetization. Focus on adoption, community, and proving the thesis. The core engine, CLI, web UI, and single-instance features are always free and open source.

### Potential Future Revenue Streams (v0.8.0+)

**1. Managed Navigator Hosting**
- Run high-availability, curated navigator nodes
- Free tier: public navigator access
- Paid tier: private navigators for organizations, SLA guarantees, dedicated infrastructure
- Why it works: Running a navigator is like running a search engine — most people don't want to

**2. Managed duh Instances (SaaS)**
- Hosted duh instances with web UI, no Docker required
- Free tier: limited threads/month, community models
- Paid tier: unlimited threads, priority model access, team features
- Why it works: Docker is a barrier for non-technical users

**3. Enterprise Features (Open Core Model)**
- SSO/SAML integration
- Audit logging
- Compliance features (data residency, retention policies)
- Priority support
- Custom navigator configuration
- Why it works: Standard open-core playbook. Core is free, enterprise features are paid.

**4. Knowledge Base Premium Access**
- Public knowledge base always free (drives adoption)
- Premium: full research reports, API access to knowledge base, priority fact-checking
- Why it works: The knowledge base becomes more valuable over time as network effects compound

### What NEVER Gets Monetized

- The core consensus engine
- The CLI tool
- Local-only operation (no network features)
- The navigator protocol (anyone can run a navigator)
- Self-hosted web UI
- Provider adapter interface (community can always add providers)

### Pricing Philosophy

- **Free forever**: Solo user, self-hosted, local models, basic features
- **Pay for convenience**: Managed hosting, no-Docker setup, automatic updates
- **Pay for scale**: Multi-user, enterprise features, SLA
- **Pay for premium content**: Curated knowledge, research reports

This aligns with core tenet 6 ("No gatekeepers") — the tool is always free. You pay for the service layer on top.

---

## 8. Risk Assessment Per Phase

### v0.1.0 - v0.3.0 Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Consensus doesn't beat single model | Medium | Critical | Protocol design matters. Forced disagreement, devil's advocate, structured challenge. If naive consensus fails, iterate on protocol before abandoning thesis. Refer to ICLR 2025 MAD evaluation — design matters more than model count. |
| API cost makes it impractical | Low | High | Cost tracking from v0.1.0. Local models from day one. Show costs, let users decide thresholds. |
| CLI adoption limited | Medium | Medium | Web UI comes at v0.5.0. But CLI-first validates with the right early adopters (power users, developers). |
| Provider API changes break adapters | Medium | Medium | Abstract adapter interface. Community can maintain adapters. Pin SDK versions. |

### v0.4.0 - v0.5.0 Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Web UI scope creep | High | Medium | Ship minimal viable UI. Real-time consensus display is the hero feature. Everything else is secondary. |
| MCP/A2A protocol instability | Medium | Low | These are integrations, not core features. Can ship behind feature flags. |

### v0.6.0 - v0.7.0 Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Federation protocol complexity | High | High | Start with simplest possible protocol. Manual navigator configuration. No auto-discovery until v1.0.0. |
| Knowledge quality in federated network | High | High | Outcome-based quality signals from day one. Bad decisions with tracked negative outcomes sink naturally. Start with curated initial navigator. |
| Privacy/trust concerns | Medium | High | Opt-in only. Per-thread control. Nothing shared without explicit user action. Default is private. |

### v0.8.0 - v1.0.0 Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Knowledge base content quality | Medium | High | Outcome tracking provides natural quality signal. Community curation. Flagging/reporting mechanism. |
| Scaling challenges | Medium | Medium | PostgreSQL path exists. Horizontal scaling designed in v0.7.0. |
| Competition catches up | Low-Medium | Medium | Knowledge accumulation and network effects create compounding advantage. The longer duh runs, the more valuable it becomes. |

---

## 9. Key Strategic Principles

1. **Ship the debate, not just the answer.** The visible consensus process IS the product. If users just wanted an answer, they'd ask one model. They come to duh to see the thinking.

2. **Local models from day one.** Not a Phase 2 afterthought. This signals commitment to the community and differentiates from cloud-only tools.

3. **Knowledge is the moat.** Models get commoditized. Orchestration gets replicated. Accumulated, consensus-validated knowledge with provenance and outcome tracking is the defensible asset.

4. **Network effects compound.** Each instance that joins the network makes every other instance more valuable. This is the long-term strategic advantage.

5. **Domain-agnostic but example-specific.** Don't market "general purpose AI." Show specific compelling debates: a business strategy question, a medical research question, a farming decision, a personal finance question. Let the breadth speak through examples.

6. **Open source is the distribution strategy.** The codebase is the marketing. GitHub stars, community contributions, and shared debates drive organic growth. Revenue comes from the service layer, never the core tool.

7. **Cost transparency builds trust.** Other tools hide costs or use opaque credit systems. duh shows every token, every model, every dollar. This builds trust with users who are spending their own money on API calls.

---

## 10. Success Metrics by Version

| Version | Primary Metric | Target | Secondary Metrics |
|---------|---------------|--------|-------------------|
| 0.1.0 | GitHub stars (first 30 days) | 500+ | HN front page, 50+ active users |
| 0.2.0 | Threads with recalled context | 30%+ of threads use past decisions | Weekly active users 200+ |
| 0.3.0 | Complex query completion rate | 80%+ successful decompositions | Avg subtasks per complex query 3-7 |
| 0.4.0 | API integrations | 10+ community integrations | MCP usage in other agents |
| 0.5.0 | Web UI daily active users | 500+ | Share link clicks 100+/day |
| 0.6.0 | Navigator-connected instances | 100+ | Network queries per day 1000+ |
| 0.7.0 | Team deployments | 20+ organizations | Multi-user instances 50+ |
| 0.8.0 | Knowledge base entries | 10,000+ | Organic search traffic 5000+/month |
| 0.9.0 | Research reports generated | 500+/month | Report shares 100+/month |
| 1.0.0 | Total active instances | 5,000+ | Network knowledge entries 100,000+ |

---

## 11. Competitive Positioning Summary

### What We Say

"duh is a thinking tool, not a workflow tool. It doesn't route your question to one model — it makes multiple models debate it, preserves the disagreements, remembers the decisions, and connects instances into a network where everyone benefits from everyone's thinking."

### What We Don't Say

- "AI agent framework" (commoditized term, wrong category)
- "LLM orchestration" (sounds like infrastructure, not a product)
- "Better than [competitor]" (different category entirely)

### Tagline Options

- "Why wouldn't you use all of them? ...duh."
- "Multi-model consensus. Persistent knowledge. Federated intelligence."
- "The thinking tool."

### Category Creation

duh doesn't fit into existing categories. It's not an agent framework, not a chatbot, not a search engine, not a knowledge base. It's a new category: **"Consensus Intelligence Infrastructure"** or, more simply, **"AI Thinking Tools."**

The goal is to own this category before anyone else defines it.

---

*This strategy document should be reviewed by the full agent team, challenged by the devil's advocate agent, and cross-referenced against the competitive landscape by the research agent before being finalized into the roadmap.*
